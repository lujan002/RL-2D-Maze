# config.yaml
ppo:
  policy: 'MlpPolicy'
  n_steps: 250
  learning_rate: 0.0003
  batch_size: 10
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: true
  sde_sample_freq: -1
  tensorboard_log: "./ppo_tensorboard/"
  seed: 123
  device: "cpu"


# policy: The policy model to use (e.g., 'MlpPolicy').
# n_steps: Number of steps to run for each environment per update. Higher steps allows the model to gather more data before each update, which can provide more stable updates but might be slower in terms of learning speed.
# learning_rate: The learning rate.
# batch_size: Minibatch size. Ensure the batch size divides evenly into n_steps. A common choice is 64, but if you have a smaller n_steps, you might want to adjust this to 32 or 16.
# n_epochs: Number of epochs when optimizing the surrogate loss. More epochs can help with better optimization but will increase computation time.
# gamma: Discount factor. Typically set to 0.99, but if your rewards are sparse, consider slightly lower values like 0.95.
# gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.
# clip_range: Clipping parameter for the surrogate loss. Typically set to 0.2. If learning is unstable, consider lowering it.
# ent_coef: Entropy coefficient for the loss calculation. A small entropy coefficient (e.g., 0.01) encourages exploration
# vf_coef: Value function coefficient for the loss calculation. Typically set to 0.5. This controls the trade-off between the value function loss and the policy loss.
# max_grad_norm: The maximum value for the gradient clipping.
# use_sde: Whether to use State Dependent Exploration (SDE).
