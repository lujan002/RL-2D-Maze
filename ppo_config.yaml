# config.yaml
ppo:
  policy: 'MlpPolicy'  # The policy model to use (e.g., 'MlpPolicy', `MlpLstmPolicy´).
  n_steps: 128  # Number of steps to run for each environment per update. Higher steps allow the model to gather more data before each update, which can provide more stable updates but might be slower in terms of learning speed.
  learning_rate: 0.0003  # The learning rate. Typical values might be 0.001 - 0.0001
  batch_size: 64  # Minibatch size. Ensure the batch size divides evenly into n_steps. A common choice is 64, but if you have a smaller n_steps, you might want to adjust this to 32 or 16.
  n_epochs: 10  # Number of epochs when optimizing the surrogate loss. More epochs can help with better optimization but will increase computation time. For simple envs, use 1-10, for complex envs, use 30-50
  gamma: 0.99  # Discount factor. Typically set to 0.99, but if your rewards are sparse, consider slightly lower values like 0.95. Increase gamma to make the agent consider future rewards more.
  gae_lambda: 0.95  # Factor for trade-off of bias vs variance for Generalized Advantage Estimator.
  clip_range: 0.2  # Clipping parameter for the surrogate loss. Typically set to 0.2. If learning is unstable, consider lowering it.
  # optional params
  ent_coef: 0.05  # Entropy coefficient for the loss calculation. A higher entropy coefficient (e.g., >0.1) encourages exploration.
  vf_coef: 0.5  # Value function coefficient for the loss calculation. Typically set to 0.5. This controls the trade-off between the value function loss and the policy loss.
  max_grad_norm: 0.5  # The maximum value for the gradient clipping.
  use_sde: true  # Whether to use State Dependent Exploration (SDE).
  sde_sample_freq: -1  # Sample a new noise matrix every n steps, when using gSDE. -1 means to sample at each step.
  target_kl: None  # Limit the KL divergence between updates, by default, there is no limit.
  policy_kwargs: 
    log_std_init: -1.0 # Reduce initial exploration noise. e.g., setting log_std_init to -2.0, which results in std ≈ 0.135
    net_arch: 
        - pi: [64, 64]
        - vf: [64, 64]
  # Additional arguments to be passed to the policy on creation.
  tensorboard_log: "./ppo_tensorboard/"  # Directory for logging TensorBoard data.
  seed: None  # Seed for the random number generators.
  device: "cuda"  # Device to use for computation ('auto', 'cpu', 'cuda', ...). Setting it to 'cpu' ensures the use of the CPU.
  _init_setup_model: True  # Whether to build the network at the creation of the model.
